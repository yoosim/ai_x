{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "739d0102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:100% !important;}\n",
       "div.cell.code_cell.rendered{width:85%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:22pt;}\n",
       "div.text_cell_render.rendered_html{font-size:18pt;}\n",
       "div.text_cell_render.rendered_html{font-size:15pt;}\n",
       "div.output {font-size:18pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:18pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:18pt;padding:5px;}\n",
       "table.dataframe{font-size:18px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:100% !important;}\n",
    "div.cell.code_cell.rendered{width:85%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:22pt;}\n",
    "div.text_cell_render.rendered_html{font-size:18pt;}\n",
    "div.text_cell_render.rendered_html{font-size:15pt;}\n",
    "div.output {font-size:18pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:18pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:18pt;padding:5px;}\n",
    "table.dataframe{font-size:18px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7ae91",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-size:35px;\"><strong> ch2. LLM활용의 기본 개념(ollama)</strong></span>\n",
    "# 1. LLM을 활용하여 답변 생성하기 \n",
    "## 1) Ollama 이용한 로컬 LLM 이용\n",
    " 성능은 GPT, Claude 같은 모델보다 떨어지나, 개념 설명을 위해 open source 모델 사용\n",
    " \n",
    "### ollama.com 다운로드 -> 설치 -> 모델 pull\n",
    "- ollama pull deepseek-r1:1.5b (window키 + R → Powershell 검색)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc6f9b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\n\\n</think>\\n\\nKorea, officially known as the Democratic People's Republic of Korea ( temporarily referred to as the Republic of Korea in some countries), has its capital city at Seoul.\", additional_kwargs={}, response_metadata={'model': 'deepseek-r1:1.5b', 'created_at': '2025-06-25T02:09:38.5588129Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3466996700, 'load_duration': 26997300, 'prompt_eval_count': 10, 'prompt_eval_duration': 77547700, 'eval_count': 38, 'eval_duration': 3362451700, 'model_name': 'deepseek-r1:1.5b'}, id='run--0bc069f3-0e7d-404f-a543-a153d91f25c7-0', usage_metadata={'input_tokens': 10, 'output_tokens': 38, 'total_tokens': 48})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "result = llm.invoke(\"What is the capital of Korea?\")\n",
    "result # 추론 모델 <think>~<think>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c7ff4",
   "metadata": {},
   "source": [
    "### ollama.com 다운로드 → 설치→ 모델 pull\n",
    "- ollama pull llama3.2:1b (window키+R → powershell 검색)\n",
    "- llama : 공식적으로 한글지원 안 됨 (llama3.1 405b한글지원 가능 -> llama3.3 70b)\n",
    "- exaone : 공식적으로 한글지원 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed7d0b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The capital of South Korea is Seoul. However, it's worth noting that the Demilitarized Zone (DMZ) surrounding the two Koreas also claims Pyongyang as its capital. The North Korean government still uses Pyongyang as its capital, while South Korea recognizes Seoul as its de facto capital. This is because the DMZ is not controlled by either side and serves as a buffer zone between the two countries.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "result = llm.invoke(\"what is the capital of Korea?\")\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9db856db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한가지 önemliething은 한국 수도입니다. 한국의 수도는 Seoul로 정기적으로 운영됩니다.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm.invoke(\"한국 수도는 어디야?\")\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db320630",
   "metadata": {},
   "source": [
    "## 2) openai 활용\n",
    "- pip install lanchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f13bf9b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m result \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is the capital of Korea?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\load\\serializable.py:130\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:690\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(\n\u001b[0;32m    684\u001b[0m             proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy, verify\u001b[38;5;241m=\u001b[39mglobal_ssl_context\n\u001b[0;32m    685\u001b[0m         )\n\u001b[0;32m    686\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    687\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client\n\u001b[0;32m    688\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _get_default_httpx_client(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_api_base, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_timeout)\n\u001b[0;32m    689\u001b[0m     }\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mOpenAI(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msync_specific)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\openai\\_client.py:126\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    124\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    127\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m     )\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# result = llm.invoke(\"what is the capital of Korea?\")\n",
    "# result\n",
    "# 에러사유 : OPENAI_API_KEY 환경변수 부재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7b20625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 환경변수 가져오기\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "# os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c5c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코랩에서 OPENAI_API_KEY 읽어오기(코랩에서는 .env를 못 씀)\n",
    "# from google.colab import userdata\n",
    "# userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da21de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=#'gpt-4.1-nano', \n",
    "                 # openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
    "llm.invoke(\"what is capital of Korea? Answer me in Korea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ecd146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 모델의 키가 OPEN_API_KEY는 아님\n",
    "# Claude -> Anthropic\n",
    "# Azure, upstage, Bedrock : 에러 메세지 참조하여 환경변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90bb3782",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for AzureOpenAI\n  Value error, Must provide either the `api_version` argument or the `OPENAI_API_VERSION` environment variable [type=value_error, input_value={'model': '', 'model_kwargs': {}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI\n\u001b[1;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mAzureOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is the capital of Korea?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\load\\serializable.py:130\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\pydantic\\main.py:253\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    252\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    255\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    259\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    260\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for AzureOpenAI\n  Value error, Must provide either the `api_version` argument or the `OPENAI_API_VERSION` environment variable [type=value_error, input_value={'model': '', 'model_kwargs': {}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
     ]
    }
   ],
   "source": [
    "# from langchain_openai import AzureOpenAI\n",
    "# llm = AzureOpenAI(model='')\n",
    "# llm.invoke(\"what is the capital of Korea?\")\n",
    "# 에러 :OPENAI_API_VERSION 환경변수가 필요하다는 메세지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71d740c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "# llm = ChatAnthropic(model='claude-3-5-sonnet-20240620')\n",
    "# llm.invoke('What is the capital of Korea?')\n",
    "# 에러 메세지를 봐도 환경변수 이름을 알 수 없음 → ChatAnthropic 검색 후, Langchain docs \n",
    "# langchain docs에서 명시한 ANTHROPIC_API_KEY이름의 환경변수 설정\n",
    "# https://python.langchain.com/docs/integrations/chat/anthropic/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afb51b2",
   "metadata": {},
   "source": [
    "# 2. 렝체인 스타일로 프롬프트 작성하기\n",
    "- 프롬프트:llm 호출시 쓰는 질문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cb40f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm=ChatOllama(model=\"llama3.2:1b\")\n",
    "# llm.invoke(0)\n",
    "# 프로프트 타입 : 스트링, PromptValue, BaseMessage 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8800b5",
   "metadata": {},
   "source": [
    "## 1)기본 프롬프트 템플릿\n",
    "- PromptTemplate을 사용하여 변수가 포함된 템플릿 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6f5eed98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='What is the capital of Korea'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "llm= ChatOllama(model=\"llama3.2:1b\")\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of {country}\", # {} 안의 값을 새로운 값으로 대입 가능 \n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "prompt = prompt_template.invoke({\"country\":\"Korea\"})\n",
    "print(prompt)\n",
    "# llm.invoke(prompt_template.invoke({\"country\":\"Korea\"})) # country에 korea "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274cb48c",
   "metadata": {},
   "source": [
    "## 2) 메세지 기반 프롬프트 작성\n",
    "- BaseMessage 리스트\n",
    "- BaseMessage 상속 받은 클래스 : AIMessage, HummanMessage, SystemMess, ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0ab951c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of Italy?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of Italy is Roma', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of Korea?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of Italy is Seoul', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# BaseMessage List로 하면 렝체인화X, ChatPromptTemplateX \n",
    "# 튜플리스트로 변경\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "message_list = [\n",
    "    SystemMessage(content=\"You are a helpful assistant\"),\n",
    "    HumanMessage(content=\"What is the capital of Italy?\"),\n",
    "    AIMessage(content=\"The capital of Italy is Roma\"),\n",
    "    HumanMessage(content=\"What is the capital of Korea?\"),\n",
    "    AIMessage(content=\"The capital of Italy is Seoul\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\")\n",
    "] # 단순한 리스트라서 템플릿을 더 거쳐야 함\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "chatpromptTemplate = ChatPromptTemplate.from_messages(message_list) # message를 프롬프트화 시킴\n",
    "prompt = chatpromptTemplate.invoke({\"country\":\"Korea\"})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eac7e4c",
   "metadata": {},
   "source": [
    "## 3)ChatPromptTemplate 사용\n",
    "- BaseMessage 리스트 → 튜플리스트 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eccd93ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어느 나라 수도가 궁금하세요?\n",
      "프롬프트 : messages=[SystemMessage(content='당신은 대한민국 전문 도우미에요!', additional_kwargs={}, response_metadata={}), HumanMessage(content='의 수도가 어디에요?', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'대한민국의 의 수도는 Seoul입니다. 그것은 주로 경인도에서 이끄는 곳이지만, 북부와 남부 cũng 일부 지역을 지배하고 있습니다.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위의 BaseMessage를 수정\n",
    "chatpromptTemplate = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"당신은 대한민국 전문 도우미에요!\"),\n",
    "    (\"human\",\"{country}의 수도가 어디에요?\"),\n",
    "    \n",
    "])\n",
    "country=input(\"어느 나라 수도가 궁금하세요?\")\n",
    "prompt = chatpromptTemplate.invoke({\"country\":country})\n",
    "print(\"프롬프트 :\",prompt)\n",
    "result = llm.invoke(prompt)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc2e1b6",
   "metadata": {},
   "source": [
    "# 3. 답변 형식을 컨트롤하기\n",
    "- invoke 실행결과는 AIMessage() → String이나 json,객체 : outputParser이용\n",
    "\n",
    "## 1) 문자열 출력 파서 이용\n",
    "- strOutputParser를 사용하여 LLM출력(AIMessage)을 단순 문자열로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8849f857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프롬프트 : text='What is the capital of korea.  Return the name of the city only'\n",
      "llm 결과 : <class 'langchain_core.messages.ai.AIMessage'> content='Seoul' additional_kwargs={} response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-25T07:05:16.5330802Z', 'done': True, 'done_reason': 'stop', 'total_duration': 724516400, 'load_duration': 26999600, 'prompt_eval_count': 41, 'prompt_eval_duration': 576951800, 'eval_count': 3, 'eval_duration': 119808100, 'model_name': 'llama3.2:1b'} id='run--39599303-5a03-4688-94d1-2ce84afbacc5-0' usage_metadata={'input_tokens': 41, 'output_tokens': 3, 'total_tokens': 44}\n",
      "파서 결과 : Seoul\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# 명시적인 지시사항이 포함된 프롬프트\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of {country}.  Return the name of the city only\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "# 프롬프트 템플릿에 값 주입\n",
    "prompt = prompt_template.invoke({\"country\":\"korea\"})\n",
    "print(\"프롬프트 :\",prompt)\n",
    "result = llm.invoke(prompt)\n",
    "print(\"llm 결과 :\",type(result), result)\n",
    "# 문자열 출력 파서를 이용하여 llm응답을 단순 문자열 반환\n",
    "output_parser = StrOutputParser()\n",
    "print(\"파서 결과 :\",output_parser.invoke(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a0bae910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(llm.invoke(prompt_template.invoke({\"country\":\"Korea\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a0b7d808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PromptTemplate(변수설정) => ChatPromptTemplate(변수설정, system과 모법답안 지정)\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant with expertise in South Korea.\"),\n",
    "    (\"human\", \"What is the capital of {country}? Return the name if the city only.\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "output_parser.invoke(llm.invoke(chat_prompt_template.invoke({\"country\":\"Korea\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0357f6",
   "metadata": {},
   "source": [
    "## 2) JSON 출력 파서 이용\n",
    "- jsom()으로 응답하기를 원하지만, 우선 어떤 형식으로 반환되는지 확인\n",
    "- {\"name\":\"홍\",\"age\":\"22\"(josn)/{\"name\":\"홍\",\"age\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ee189281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.StringPromptValue'> text='Give Following information about Korea\\n    - Capital\\n    - Popultion\\n    - Languate\\n    - Currency\\n    return it is JSON format and return the JSON dictionart only'\n",
      "<class 'langchain_core.messages.ai.AIMessage'> content='```\\n{\\n    \"capital\": \"Seoul\",\\n    \"population\": 51.8,\\n    \"language\": \"Korean (based on Hangul)\",\\n    \"currency\": \"South Korean Won\"\\n}\\n```' additional_kwargs={} response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-25T07:06:08.9298263Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4060633700, 'load_duration': 30070200, 'prompt_eval_count': 65, 'prompt_eval_duration': 1021683500, 'eval_count': 45, 'eval_duration': 3008112800, 'model_name': 'llama3.2:1b'} id='run--76248fb7-609b-4bb5-8733-fcb4a36bb324-0' usage_metadata={'input_tokens': 65, 'output_tokens': 45, 'total_tokens': 110}\n",
      "<class 'dict'> {'capital': 'Seoul', 'population': 51.8, 'language': 'Korean (based on Hangul)', 'currency': 'South Korean Won'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "count_detail_prompr = PromptTemplate(\n",
    "    template = \"\"\"Give Following information about {country}\n",
    "    - Capital\n",
    "    - Popultion\n",
    "    - Languate\n",
    "    - Currency\n",
    "    return it is JSON format and return the JSON dictionart only\"\"\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "prompt = count_detail_prompr.invoke({\"country\":\"Korea\"})\n",
    "print(type(prompt),prompt)\n",
    "# Json output 파서\n",
    "output_parser = JsonOutputParser()\n",
    "ai_message = llm.invoke(prompt)\n",
    "print(type(ai_message), ai_message)\n",
    "json_result = output_parser.invoke(ai_message)\n",
    "print(type(json_result),json_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "238934ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capital': 'Seoul',\n",
       " 'population': 51.8,\n",
       " 'language': 'Korean',\n",
       " 'currency': 'Korean Won'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_detail_prompr = PromptTemplate(\n",
    "    template = \"\"\"Give Following information about {country}\n",
    "    - Capital\n",
    "    - Popultion\n",
    "    - Language\n",
    "    - Currency\n",
    "    return it is JSON format and return the JSON dictionart only\"\"\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "output_parser = JsonOutputParser()\n",
    "info = output_parser.invoke(llm.invoke(count_detail_prompr.invoke({\"country\":\"Korea\"})))\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "88e4befe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6545e1",
   "metadata": {},
   "source": [
    "## 3) 구조화된 출력 사용\n",
    "- Pydantic 모델을 사용하여 LLM 출력을 구조화된 형식으로 받기(Jsonparser보다 훨씬 안정적)\n",
    "- Pydantic : 데이터 유효성 검사, 설정관리를 간편하게 해주는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7b5f33e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.User object at 0x0000021BCC5879D0>\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel,Field\n",
    "class User:\n",
    "    def __init__(self, id, name, is_active=True):\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.is_active = is_active\n",
    "#         if len(name)<=1:\n",
    "#             throw Exception \n",
    "#     def __str__(self):\n",
    "#         return self.name + \" \" + self.id\n",
    "user = User(1,\"홍길동\")\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7f54372a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=1 name='홍길동' is_active=True\n"
     ]
    }
   ],
   "source": [
    "class User(BaseModel):\n",
    "    # gt=0:id>0 / ge=0:id>=0 / lt=0:id<0 / le=0:id<=0\n",
    "    id:int     = Field(gt=0, description=\"id\") # 미니멈 값\n",
    "    name:str   = Field(min_length=2, description=\"name\")\n",
    "    is_active:bool=Field(default=True,description=\"id활성화\")\n",
    "user = User(id=\"1\", name=\"홍길동\")\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "849469c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountryDetail(capital='Seoul', population=51000000, language='Korean', currency='Korean won')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_detail_prompr = PromptTemplate(\n",
    "    template = \"\"\"Give Following information about {country}\n",
    "    - Capital\n",
    "    - Popultion\n",
    "    - Language\n",
    "    - Currency\n",
    "    return it is JSON format and return the JSON dictionart only\"\"\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "class CountryDetail(BaseModel): # description : 더 정확한 출력 유도\n",
    "    capital:str = Field(description=\"the capital of the country\")\n",
    "    population:int = Field(description=\"the population of the country\")\n",
    "    language:str = Field(description=\"the language of the country\")\n",
    "    currency:str = Field(description=\"the capital of the country\")\n",
    "# 출력 형식 파서 생성 (llm과 파싱이 같이 되는 기능)\n",
    "structedllm = llm.with_structured_output(CountryDetail)\n",
    "\n",
    "# 기존 방식 \n",
    "# output_parser = JsonOutputParser()\n",
    "# output_parser.invoke(llm.invoke(count_detail_prompr.invoke({\"country\":\"Korea\"})))\n",
    "\n",
    "info = structedllm.invoke(count_detail_prompr.invoke({\"country\":\"Korea\"}))\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4ad32b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.CountryDetail"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(info) # 타입은 객체로 받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "de7f9436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital='Seoul' population=51000000 language='Korean' currency='Korean won'\n",
      "Seoul 51000000 Korean Korean won\n"
     ]
    }
   ],
   "source": [
    "print(info)\n",
    "print(info.capital, info.population, info.language, info.currency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4872d3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info → JSON : {\"capital\":\"Seoul\",\"population\":51000000,\"language\":\"Korean\",\"currency\":\"Korean won\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6456\\2195821428.py:1: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  print('info → JSON :', info.json())\n"
     ]
    }
   ],
   "source": [
    "print('info → JSON :', info.json()) # 곧 없어질 예정이라고 경고 발생 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e7293abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info → JSON : {\"capital\":\"Seoul\",\"population\":51000000,\"language\":\"Korean\",\"currency\":\"Korean won\"}\n"
     ]
    }
   ],
   "source": [
    "print('info → JSON :', info.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "576e50a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info → dict : {'capital': 'Seoul', 'population': 51000000, 'language': 'Korean', 'currency': 'Korean won'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6456\\1042155660.py:1: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  print('info → dict :', info.dict())\n"
     ]
    }
   ],
   "source": [
    "print('info → dict :', info.dict()) # 곧 없어질 예정이라고 경고 발생 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0eccb7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info → dict : {'capital': 'Seoul', 'population': 51000000, 'language': 'Korean', 'currency': 'Korean won'}\n"
     ]
    }
   ],
   "source": [
    "print('info → dict :', info.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87171db",
   "metadata": {},
   "source": [
    "# 4. LCEL을 활용한 랭체인 생성하기\n",
    "## 1) 문자열 출력 파서 사용\n",
    "- invoke "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "166015ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3.2:1b\",\n",
    "                 temperature=0) # 일관된 답변\n",
    "# 명시적인 지시사항이 포함된 프롬프트\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of {country}.  Return the name of the city only\",\n",
    "    input_variables = [\"country\"])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "output_parser.invoke(llm.invoke(prompt_template.invoke({\"country\":\"Korea\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a85e15",
   "metadata": {},
   "source": [
    "invoke를 계속 사용하며 문장구성이 어지러움/ LCEL을 쓰면 invoke를 쓰지않고 연산자를 사용함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d8340",
   "metadata": {},
   "source": [
    "## 2) LCEL을 사용한 간단한 체인 구성\n",
    "- 파이프연산자(|) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ae53b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 프롬프트템플릿 → llm → 출력파서를 연결하는 체인 생성 \n",
    "capital_chain = prompt_template | llm | output_parser\n",
    "# 생성된 체인을 invoke\n",
    "capital_chain.invoke({\"country\":\"Korea\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bda40c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(capital_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee64f75",
   "metadata": {},
   "source": [
    "## 3) 복합 체인 구성\n",
    "- 여러단계의 추론이 필요한 경우 (체인 연결)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee1dfbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italy.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나라 설명(입력) → 나라명(출력)\n",
    "from langchain_core.prompts import PromptTemplate \n",
    "# 페르소나 지정X,ChatPromptTemplate는 페르소나 지정 \n",
    "country_prompt = PromptTemplate(\n",
    "    template = \"\"\"Guess the name of the country based on the following information: \n",
    "    {information}\n",
    "    Return the name of the the country only\"\"\",\n",
    "    imput_variavles=[\"information\"]\n",
    ")\n",
    "output_parser.invoke(llm.invoke(country_prompt.invoke({\"information\":\n",
    "                                                       \"This country is very famous for its wine\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ad4f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italy.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나라명 추측 체인 생성\n",
    "country_chain = country_prompt | llm | output_parser\n",
    "# type(country_chain)\n",
    "country_chain.invoke({\"information\":\"This country is very famous for its wine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ca7cbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rome'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나라 설명(입력) → (나라명 → ) → 나라 수도(출력)\n",
    "final_chain = country_chain | capital_chain \n",
    "final_chain.invoke({\"information\":\"This country is very famous for its wine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d58e8789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rome'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain = {\"country\" : country_chain} | capital_chain\n",
    "final_chain.invoke({\"information\":\"This country is very famous for its wine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "541f4a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rome'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "final_chain = {\"information\":RunnablePassthrough()} | {\"country\":country_chain} | capital_chain\n",
    "final_chain.invoke(\"This country is very famous for its wine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb12acaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italy'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 프롬프트 템플릿에 변수2 \n",
    "# 나라설명 → 나라명\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "country_prompt = PromptTemplate(\n",
    "    template =\"\"\"Guess the name of the country in the {continent} based\n",
    "    on the following information:{information}\n",
    "    Return the name of the country only\"\"\",\n",
    "    input_variables=[\"information\",\"continent\"])\n",
    "\n",
    "country_chain = country_prompt | llm | output_parser\n",
    "country_chain.invoke({\"information\":\"This country is very famous for its wine\",\n",
    "                      \"continent\":\"Europe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf2c0cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rome'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain = {\"country\" : country_chain} | capital_chain\n",
    "final_chain.invoke({\"information\":\"This country is very famous for its wine\",\n",
    "                    \"continent\":\"Europe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec40253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. 나라명(입력) → (제일 유명한 음식 → ) → 음식의 레시피(출력)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d521e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f694f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f627e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7ce74b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e5259a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8ea8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a128ca96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21adc512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163d9111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa66574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725f9569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1dc343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aef3268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm(ipykernel)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233.977px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
